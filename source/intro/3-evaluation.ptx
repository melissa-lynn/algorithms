<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="intro-3-evaluation" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Evaluating an Algorithm</title>

  <p>Suppose we have two different algorithms for the same task. For example, there are many different algorithms for sorting an array. We've seen merge sort, which sorts an array by sorting the first half and the second half separately, then merging the two sorted halves. We repeat the pseudocode for merge sort here.</p>
  
  <program>
    merge_sort

    input: array of integers of length n
    state change: array of integers sorted into ascending order

    if the length of the array is 0 or 1:
      return

    call merge_sort to sort the first half of array
    call merge_sort to sort the second half of the array

    new_array is initialized as an empty array

    iterate through the first half and second half of the array simultaneously
      if the current item in the first half is less than the current item in the second half:
        append the current item from the first half to new_array
        go to the next item in the first half
      else:
        append the current item from the second half to new_array
        go to the next item in the second half

    if there are remaining items in the first half of the array:
      append them to new_array

    if there are remaining items in the second half of the array:
      append them to new_array
    
    copy new_array into the original array

    return
  </program>
  
  <p>Selection sort provides another approach to sorting: we repeatedly find the next smallest item in the array, and move it into the correct position. Here is pseudocode for selection sort.</p>

  <program>
    selection_sort

    input: array of integers of length n
    state change: array of integers sorted into ascending order

    for each position in the array:
      find the smallest item in the array, at or after the current position
      swap the item at the current position with the item just found

    return
  </program>

  <p>We now have two different different algorithms for sorting an array, but which one is better? To answer this question, we need some idea of what makes an algorithm <q>good</q> in general, or how to compare two algorithms. When evaluating an algorithm, we typically focus on <em>correctness</em> and <em>efficiency</em></p>

  <principle xml:id="principle-correctness">
    <title>Correctness</title>
    <statement>
      <p>
        Does the algorithm always give correct results? If not, how often does it give correct results, and how close to correct are the results?
      </p>
    </statement>
  </principle>

  <p>To evaluate the correctness of an algorithm, we typically adopt one of the following two approaches.
    <ul>
      <li>
        <p>
          Write test cases with expected output, and run an implementation of the algorithm on these test cases to verify that the algorithm produces the expected output. The challenge of this approach is writing test cases that cover all possible situations. If we miss a situation in our test cases, we may think our algorithm is correct when it is not. One of the benefits of this approach is that test cases can be written before the algorithm is written, and they can be used to evaluate any algorithm accomplishing the same task. Test cases can also be useful in developing the algorithm itself.
        </p>
      </li>
      <li>
        <p>
          Prove mathematically that the algorithm does what is expected. The benefit of this approach is that if you have a rigorous proof, you can be certain your algorithm is correct. The challenge is that writing and communicating these proofs can be difficult, and in some cases test cases may provide more compelling evidence that an algorithm works as intended.
        </p>
      </li>
    </ul>
  </p>

  <p>For sorting algorithms: test cases and proofs of correctness.</p>

  <example>
    <title>Test Cases for Sorting Algorithms</title>
    <statement>
      <p>Discussion of test cases</p>
    </statement>
  </example>

  <example>
    <title>Proof of Correctness of Merge Sort</title>
    <statement>
      <p>Proof</p>
    </statement>
  </example>

  <example>
    <title>Proof of Correctness of Selection Sort</title>
    <statement>
      <p>Discussion of test cases</p>
    </statement>
  </example>

  <principle xml:id="principle-efficiency">
    <title>Efficiency</title>
    <statement>
      <p>
        What are the resource requirements of the algorithm? In particular, how long does the algorithm take to run (<em>time efficiency</em>), and how much memory does the algorithm require (<em>space efficiency</em>)? We are especially concerned with <em>scalability</em> (how well the algorithm works on very large inputs).
      </p>
    </statement>
  </principle>

  <p>To evaluate the time efficiency of an algorithm with an emphasis on scalability, we could run an implementation of the algorithm on inputs of various sizes, and time how long the algorithm takes to run. However, this approach has significant drawbacks. The time an algorithm takes to run may vary significantly depending on the power of the computer used, or on the programming language used to implement the algorithm. With this approach, to compare the time efficiency of two algorithms, we would need to be sure to implement them in the same programming language and run them on the same computer. It would be preferable to have an approach that measures the time efficiency intrinsic to an algorithm, without depending on a specific language or hardware.</p>

  <p>Similarly, to evaluate the space efficiency of an algorithm, we could track the memory usage of the algorithm for inputs of various sizes. Again, this could depend on the programming language used, and the type of memory used could depend on the hardware configuration (for example, if the algorithm fills up the cache). As with time efficiency, we would like to be able to evaluate the space requirements of an algorithm without focusing on a specific implementation.</p>

  <p>In order to more universally describe the time and space efficiency of an algorithm, we will use asymptotics. This approach allows us to describe the growth of the time and space requirements of an algorithm as input size increases, in a way that does not depend on a specific implementation of the algorithm. You may have seen this before as the <q>big O</q> of an algorithm.</p>

  <example>
    <title>Timing Sorting Algorithms</title>
    <statement>
      <p>Discussion of timing experiment</p>
    </statement>
  </example>

  <example>
    <title>Measuring Memory Use of Sorting Algorithms</title>
    <statement>
      <p>Discussion of memory analysis</p>
    </statement>
  </example>

  <example>
    <title>Runtime Analysis of Merge Sort</title>
    <statement>
      <p>Not sure if this should be included here</p>
    </statement>
  </example>

  <example>
    <title>Runtime Analysis of Selection Sort</title>
    <statement>
      <p>Not sure if this should be included here</p>
    </statement>
  </example>

</section>